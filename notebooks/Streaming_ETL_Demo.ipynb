{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f34bbac",
   "metadata": {},
   "source": [
    "# Streaming ETL Pipeline\n",
    "\n",
    "This notebook executes the full ETL process using the project’s Python modules.\n",
    "\n",
    "The workflow includes:\n",
    "\n",
    "- **Extract:** load the local Netflix demo dataset  \n",
    "- **Transform:** clean and normalize the movie entries  \n",
    "- **Load:** apply SQL schema and insert data into Neon PostgreSQL  \n",
    "- **Validate:** verify database content with SQL queries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e5fba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. INITIAL SETUP\n",
    "# ============================================================\n",
    "\n",
    "# Load environment variables and verify that database credentials are available.\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "\n",
    "if not DATABASE_URL:\n",
    "    raise ValueError(\"DATABASE_URL not found in .env\")\n",
    "\n",
    "DATABASE_URL\n",
    "# Load environment variables and verify that database credentials are available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a86c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. HELPER FUNCTION (RUN ETL SCRIPTS)\n",
    "# ============================================================\n",
    "\n",
    "# To keep the notebook clean and readable, we use a small utility that executes any ETL script by name.\n",
    "#This mirrors real engineering notebooks, where orchestration remains minimal while the logic resides in separate modules.\n",
    "\n",
    "import subprocess, sys, os\n",
    "\n",
    "PROJECT_ROOT = os.path.dirname(os.getcwd())\n",
    "\n",
    "def run(script):\n",
    "    script_path = os.path.join(PROJECT_ROOT, \"etl\", script)\n",
    "    print(f\"\\n▶ Running: {script_path}\")\n",
    "\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, script_path],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        encoding=\"utf-8\",\n",
    "        errors=\"replace\"\n",
    "    )\n",
    "\n",
    "    print(result.stdout)\n",
    "    if result.stderr.strip():\n",
    "        print(\"Errors:\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43e849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. TEST DATABASE CONNECTION\n",
    "# ============================================================\n",
    "\n",
    "#We begin by confirming that Neon PostgreSQL is reachable.  \n",
    "#This ensures the ETL can run safely. \n",
    "\n",
    "run(\"test_connection.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f0fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. CLEAN & TRANSFORM DATASET\n",
    "# ============================================================\n",
    "\n",
    "#We clean the raw Netflix dataset and prepare the final structure used in the analytics pipeline.\n",
    "\n",
    "run(\"clean_local_netflix_csv.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567dac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. RESET DATABASE SCHEMA\n",
    "# ============================================================\n",
    "\n",
    "#Before loading new data, we drop all existing tables to ensure a clean, reproducible ETL environment.\n",
    "\n",
    "run(\"reset_schema.py\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f1e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. APPLY SQL SCHEMA (DDL)\n",
    "# ============================================================\n",
    "\n",
    "#We now create the SQL structure for the `movies` table using the schema defined in:\n",
    "\n",
    "# ``sql/000_schema.sql``\n",
    "\n",
    "run(\"apply_schema.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390c339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. LOAD CLEANED DATA INTO NEON\n",
    "# ============================================================\n",
    "\n",
    "#The cleaned dataset is inserted into the Neon PostgreSQL database using SQLAlchemy.\n",
    "\n",
    "run(\"etl_load_movies.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b935057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9. VALIDATE FINAL TABLE\n",
    "# ============================================================\n",
    "\n",
    "#Finally, we query the database to confirm that the `movies` table has been generated successfully.\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM movies LIMIT 20;\", engine)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41f2ba5",
   "metadata": {},
   "source": [
    "# CONCLUSION\n",
    "The ETL pipeline executed successfully:\n",
    "\n",
    "- Extract: dataset loaded from local CSV  \n",
    "- Transform: cleaned into 7,973 normalized rows  \n",
    "- Load: stored into Neon PostgreSQL  \n",
    "- Validate: confirmed via SQL query  \n",
    "\n",
    "This notebook demonstrates a clean, modular, and production-style ETL workflow.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
